{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import pickle\n",
    "import pprint\n",
    "import chardet\n",
    "import warnings\n",
    "import urllib.request\n",
    "from telegram import Bot\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from heapq import nlargest\n",
    "from bs4 import BeautifulSoup\n",
    "from socket import timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pretty printer\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initilize telegram bot\n",
    "token = \"350553078:AAEu70JDqMFcG_x5eBD3nqccTvc4aFNMKkg\"\n",
    "chat_id = \"126551968\"\n",
    "bot = Bot(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dataset file paths\n",
    "dataset_path = 'data/net_dbis/'\n",
    "authors_csv_path = dataset_path + 'id_author.txt'\n",
    "conferences_csv_path = dataset_path + 'id_conf.txt'\n",
    "papers_csv_path = dataset_path + 'paper.txt'\n",
    "paper_author_edges_csv_path = dataset_path + 'paper_author.txt'\n",
    "paper_conference_edges_csv_path = dataset_path + 'paper_conf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#detect encodings of files\n",
    "encodings = {}\n",
    "file_paths = [authors_csv_path, conferences_csv_path, papers_csv_path, paper_author_edges_csv_path, paper_conference_edges_csv_path]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        encodings[file_path] = (chardet.detect(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store cvs contents in dataframe\n",
    "authors_df = pd.read_csv(authors_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[authors_csv_path][\"encoding\"])\n",
    "conferences_df = pd.read_csv(conferences_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[conferences_csv_path][\"encoding\"])\n",
    "papers_df = pd.read_csv(papers_csv_path, sep='     ', header=None, dtype={0:str, 1:str}, encoding=encodings[papers_csv_path][\"encoding\"])\n",
    "paper_author_edges_df = pd.read_csv(paper_author_edges_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[paper_author_edges_csv_path][\"encoding\"])\n",
    "paper_conference_edges_df = pd.read_csv(paper_conference_edges_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[paper_conference_edges_csv_path][\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give authors, papers and conferences unique node-ids\n",
    "authors_df[0] = 'a' + authors_df[0]\n",
    "conferences_df[0] = 'c' + conferences_df[0]\n",
    "papers_df[0] = 'p' + papers_df[0]\n",
    "paper_author_edges_df[0] = 'p' + paper_author_edges_df[0]\n",
    "paper_author_edges_df[1] = 'a' + paper_author_edges_df[1]\n",
    "paper_conference_edges_df[0] = 'p' + paper_conference_edges_df[0]\n",
    "paper_conference_edges_df[1] = 'c' + paper_conference_edges_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define networkx graph\n",
    "dbis_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define node and edge label constants\n",
    "AUTHOR = 'author'\n",
    "PAPER = 'paper'\n",
    "CONFERENCE = 'conference'\n",
    "PUBLISHED_AT = 'published_at'\n",
    "WRITTEN_BY = 'written_by' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60694 nodes in graph\n",
      "133596 nodes in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add author, paper and conference nodes to graph\n",
    "dbis_graph.add_nodes_from(authors_df[0].tolist(), label=AUTHOR)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))\n",
    "dbis_graph.add_nodes_from(papers_df[0].tolist(), label=PAPER)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))\n",
    "dbis_graph.add_nodes_from(conferences_df[0].tolist(), label=CONFERENCE)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create edge tuples from dataframe\n",
    "paper_author_edges = list(zip(paper_author_edges_df[0].tolist(), paper_author_edges_df[1].tolist()))\n",
    "paper_conference_edges = list(zip(paper_conference_edges_df[0].tolist(), paper_conference_edges_df[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72902 edges in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add (paper)-[published_at]-(conference) edges to graph\n",
    "dbis_graph.add_edges_from(paper_conference_edges, label=PUBLISHED_AT)\n",
    "print(\"{} edges in graph\".format(dbis_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265317 edges in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add (paper)-[written_by]-(author) edges to graph\n",
    "dbis_graph.add_edges_from(paper_author_edges, label=WRITTEN_BY)\n",
    "print(\"{} edges in graph\".format(dbis_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_scholar_paper(paper_index):\n",
    "    url = \"http://citeseerx.ist.psu.edu/search?q=\"\n",
    "    q = \"title%3A%28{}%29&submit=Search&sort=cite&t=doc\"\n",
    "    title = papers_df.loc[paper_index, 1].strip()[:-1]\n",
    "    url += q.format(title.replace (\" \", \"+\"))\n",
    "    \n",
    "    cited_by = -1\n",
    "    try:\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(url, timeout=10).read())\n",
    "    except timeout:\n",
    "        cited_by = -2\n",
    "    \n",
    "    if soup.html.body(\"div\", id = \"result_list\")[0].find(\"div\", class_=\"error\") is None:\n",
    "        result = soup.html.body(\"div\", id = \"result_list\")[0].find(\"div\", class_=\"pubextras\").find(\"a\", class_=\"citation\").string\n",
    "        cited_by = int(result.split(\"Cited by \")[-1].split(\" (\")[0])\n",
    "    \n",
    "    return cited_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already crawled 1 papers\n",
      "Already crawled 11 papers\n",
      "Already crawled 21 papers\n"
     ]
    }
   ],
   "source": [
    "papers_citations_count = {}\n",
    "start_time = time.time()\n",
    "\n",
    "with Pool(cpu_count() * 40) as pool:\n",
    "    for i, result in enumerate(pool.imap(crawl_scholar_paper, list(range(500)), chunksize=1)):\n",
    "        if i % 10 == 0:\n",
    "            print(\"Already crawled {} papers\".format(i+1))\n",
    "        papers_citations_count[papers_df.loc[i,1].strip()] = result\n",
    "\n",
    "end_time = time.time()\n",
    "crawl_duration = round(end_time - start_time,2)\n",
    "print(\"Crawling took {} sec.\".format(crawl_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
