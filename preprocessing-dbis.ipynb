{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import pickle\n",
    "import pprint\n",
    "import chardet\n",
    "from telegram import Bot\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPORT_AS_EDGE_LIST = True\n",
    "EXTRACT_SUB_GRAPH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pretty printer\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initilize telegram bot\n",
    "token = \"350553078:AAEu70JDqMFcG_x5eBD3nqccTvc4aFNMKkg\"\n",
    "chat_id = \"126551968\"\n",
    "bot = Bot(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dataset file paths\n",
    "dataset_path = 'data/net_dbis/'\n",
    "authors_csv_path = dataset_path + 'id_author.txt'\n",
    "conferences_csv_path = dataset_path + 'id_conf.txt'\n",
    "papers_csv_path = dataset_path + 'paper.txt'\n",
    "paper_author_edges_csv_path = dataset_path + 'paper_author.txt'\n",
    "paper_conference_edges_csv_path = dataset_path + 'paper_conf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#detect encodings of files\n",
    "encodings = {}\n",
    "file_paths = [authors_csv_path, conferences_csv_path, papers_csv_path, paper_author_edges_csv_path, paper_conference_edges_csv_path]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        encodings[file_path] = (chardet.detect(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# store cvs contents in dataframe\n",
    "authors_df = pd.read_csv(authors_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[authors_csv_path][\"encoding\"])\n",
    "conferences_df = pd.read_csv(conferences_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[conferences_csv_path][\"encoding\"])\n",
    "papers_df = pd.read_csv(papers_csv_path, sep='     ', header=None, dtype={0:str, 1:str}, encoding=encodings[papers_csv_path][\"encoding\"])\n",
    "paper_author_edges_df = pd.read_csv(paper_author_edges_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[paper_author_edges_csv_path][\"encoding\"])\n",
    "paper_conference_edges_df = pd.read_csv(paper_conference_edges_csv_path, sep='\\t', header=None, dtype={0:str, 1:str}, encoding=encodings[paper_conference_edges_csv_path][\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give authors, papers and conferences unique node-ids\n",
    "authors_df[0] = 'a' + authors_df[0]\n",
    "conferences_df[0] = 'c' + conferences_df[0]\n",
    "papers_df[0] = 'p' + papers_df[0]\n",
    "paper_author_edges_df[0] = 'p' + paper_author_edges_df[0]\n",
    "paper_author_edges_df[1] = 'a' + paper_author_edges_df[1]\n",
    "paper_conference_edges_df[0] = 'p' + paper_conference_edges_df[0]\n",
    "paper_conference_edges_df[1] = 'c' + paper_conference_edges_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define networkx graph\n",
    "dbis_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define node and edge label constants\n",
    "AUTHOR = 'author'\n",
    "PAPER = 'paper'\n",
    "CONFERENCE = 'conference'\n",
    "PUBLISHED_AT = 'published_at'\n",
    "WRITTEN_BY = 'written_by' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60694 nodes in graph\n",
      "133596 nodes in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add author, paper and conference nodes to graph\n",
    "dbis_graph.add_nodes_from(authors_df[0].tolist(), label=AUTHOR)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))\n",
    "dbis_graph.add_nodes_from(papers_df[0].tolist(), label=PAPER)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))\n",
    "dbis_graph.add_nodes_from(conferences_df[0].tolist(), label=CONFERENCE)\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create edge tuples from dataframe\n",
    "paper_author_edges = list(zip(paper_author_edges_df[0].tolist(), paper_author_edges_df[1].tolist()))\n",
    "paper_conference_edges = list(zip(paper_conference_edges_df[0].tolist(), paper_conference_edges_df[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72902 edges in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add (paper)-[published_at]-(conference) edges to graph\n",
    "dbis_graph.add_edges_from(paper_conference_edges, label=PUBLISHED_AT)\n",
    "print(\"{} edges in graph\".format(dbis_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265317 edges in graph\n",
      "134060 nodes in graph\n"
     ]
    }
   ],
   "source": [
    "# add (paper)-[written_by]-(author) edges to graph\n",
    "dbis_graph.add_edges_from(paper_author_edges, label=WRITTEN_BY)\n",
    "print(\"{} edges in graph\".format(dbis_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract top-5000 authors with regard to number of publications\n",
    "# add each author with less than 8 papers to the delete candidates\n",
    "if EXTRACT_SUB_GRAPH:\n",
    "    num_of_top_k_authors = 5000\n",
    "    author_degrees = []\n",
    "    for node in list(dbis_graph.nodes):\n",
    "        if dbis_graph.nodes[node]['label'] == AUTHOR:\n",
    "            author_degrees.append(dbis_graph.degree(node))\n",
    "\n",
    "    top_k_author_degree_threshold = min(nlargest(num_of_top_k_authors, author_degrees))\n",
    "    delete_candidates = []\n",
    "\n",
    "    for node in list(dbis_graph.nodes):\n",
    "        if dbis_graph.nodes[node]['label'] == AUTHOR:\n",
    "            if dbis_graph.degree(node) <= top_k_author_degree_threshold:\n",
    "                delete_candidates.append(node)\n",
    "\n",
    "    print(\"{} authors with less than {} papers are delete candidates\".format(len(delete_candidates),top_k_author_degree_threshold+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author delete candidates from graph\n",
    "if EXTRACT_SUB_GRAPH:\n",
    "    dbis_graph.remove_nodes_from(delete_candidates)\n",
    "    print(\"{} edges in graph\".format(dbis_graph.number_of_edges()))\n",
    "    print(\"{} nodes in graph\".format(dbis_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export graph as edge list to given path\n",
    "if EXPORT_AS_EDGE_LIST:\n",
    "    edge_list_export_path = dataset_path + 'dbis_edgelist.csv'\n",
    "    nx.write_edgelist(dbis_graph, edge_list_export_path, data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/net_dbis/dbis_mapping_ids_to_nodes.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8e44ce34c64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mid2node_filepath\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'dbis_mapping_ids_to_nodes.p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mid_2_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2node_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mid_2_node_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mid_2_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_2_node_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/net_dbis/dbis_mapping_ids_to_nodes.p'"
     ]
    }
   ],
   "source": [
    "# load id-to-node mapping of verse embeddings\n",
    "id2node_filepath =  dataset_path + 'dbis_mapping_ids_to_nodes.p'\n",
    "id_2_node = {}\n",
    "with open(id2node_filepath, 'rb') as id_2_node_file:\n",
    "    id_2_node = pickle.load(id_2_node_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load node-to-id mapping of verse embeddings\n",
    "node2id_filepath = dataset_path + 'dbis_mapping_nodes_to_ids.p'\n",
    "node_2_id = {}\n",
    "with open(node2id_filepath, 'rb') as node_2_id_file:\n",
    "    node_2_id = pickle.load(node_2_id_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg. node degree is 3.96\n"
     ]
    }
   ],
   "source": [
    "# compute average degree of all nodes in graph\n",
    "node_degrees = np.array(list(dict(dbis_graph.degree(list(dbis_graph.nodes))).values()),dtype=np.int64)\n",
    "avg_node_degree = np.mean(node_degrees)\n",
    "print(\"The avg. node degree is {}\".format(np.round(avg_node_degree, decimals=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define random walk hyper-parameters\n",
    "sim_G_sampling = {}\n",
    "samples_per_node = 10000\n",
    "finished_nodes = 0\n",
    "experiment_name = 'DBIS Node Sampling V1'\n",
    "SEND_NOTIFICATIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define meta-path scoring information\n",
    "\n",
    "meta_path_scheme_A = [AUTHOR, WRITTEN_BY, PAPER, WRITTEN_BY, AUTHOR]\n",
    "meta_path_scheme_B = [AUTHOR, WRITTEN_BY, PAPER, PUBLISHED_AT, CONFERENCE, PUBLISHED_AT, PAPER, WRITTEN_BY, AUTHOR]\n",
    "meta_path_scheme_C = [PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER]\n",
    "meta_path_scheme_D = [PAPER, PUBLISHED_AT, CONFERENCE, PUBLISHED_AT, PAPER]\n",
    "meta_path_scheme_E = [PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER]\n",
    "meta_path_scheme_F = [PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER]\n",
    "meta_path_scheme_G = [CONFERENCE, PUBLISHED_AT, PAPER, WRITTEN_BY, AUTHOR, WRITTEN_BY, PAPER, PUBLISHED_AT, CONFERENCE]\n",
    "\n",
    "meta_path_schemes = {\n",
    "    AUTHOR: [meta_path_scheme_A, meta_path_scheme_B],\n",
    "    PAPER: [meta_path_scheme_C, meta_path_scheme_D, meta_path_scheme_E, meta_path_scheme_F],\n",
    "    CONFERENCE: [meta_path_scheme_G]}\n",
    "scoring_function = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample a meta-path scheme from all meta-path schemes according to given scoring function\n",
    "def sample_meta_path_scheme(node):\n",
    "    node_label = dbis_graph.nodes[node]['label']\n",
    "    meta_path_scheme_index = np.random.choice(list(range(len(meta_path_schemes[node_label]))))\n",
    "    meta_path_scheme = meta_path_schemes[node_label][meta_path_scheme_index]\n",
    "    \n",
    "    return meta_path_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check, wheter neighbor (candidate) of node i in walk fulfills requirements given through meta-path scheme\n",
    "def candidate_valid(node, candidate, meta_path_scheme,step):\n",
    "    node_label_valid = dbis_graph.nodes[candidate]['label'] == meta_path_scheme[(step+1)*2-2]\n",
    "    edge_label_valid = dbis_graph[node][candidate]['label'] == meta_path_scheme[(step+1)*2-3]\n",
    "    \n",
    "    return node_label_valid and edge_label_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute transition probabilities for all neighborhood nodes of node i according to given meta-path\n",
    "def compute_transition_probabilities(meta_path_scheme, step, node):\n",
    "    candidate_set = list(dbis_graph[node])\n",
    "    transition_probabilities = np.ones(len(candidate_set), dtype=np.float64)\n",
    "    for i, candidate in enumerate(candidate_set):\n",
    "        if not candidate_valid(node, candidate, meta_path_scheme, step):\n",
    "            transition_probabilities[i] = 0\n",
    "            \n",
    "    transition_probabilities = transition_probabilities / np.sum(transition_probabilities)\n",
    "    \n",
    "    return transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run single random walk with transistion probabilities accoring to scoring function\n",
    "def run_single_random_walk(start_node):\n",
    "    current_node = start_node\n",
    "    meta_path_scheme = sample_meta_path_scheme(start_node)\n",
    "    nodes_in_meta_path = int((len(meta_path_scheme) + 1) / 2)\n",
    "\n",
    "    for i in range(1,nodes_in_meta_path): \n",
    "        transition_probabilities = compute_transition_probabilities(meta_path_scheme, i, current_node)\n",
    "        if np.sum(transition_probabilities) == 0:\n",
    "            return current_node\n",
    "        current_node = np.random.choice([n for n in dbis_graph.neighbors(current_node)], p=transition_probabilities)\n",
    "        \n",
    "    return current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample 10.000 times a similar node given particular node\n",
    "def create_samples_for_node(node):\n",
    "    sampled_nodes = []\n",
    "    \n",
    "    for i in range(samples_per_node):\n",
    "        sampled_nodes.append(run_single_random_walk(node))\n",
    "        \n",
    "    return sampled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole sampling process took 40.36 sec.\n"
     ]
    }
   ],
   "source": [
    "# sample 10.000 similar nodes for each node in node_list in parallel\n",
    "num_node_partitions = 5\n",
    "num_nodes_per_partition = int(dbis_graph.number_of_nodes() / num_node_partitions)\n",
    "partition_id = 3\n",
    "\n",
    "lower_partition_index = partition_id * num_nodes_per_partition\n",
    "upper_partition_index = (partition_id + 1) * num_nodes_per_partition\n",
    "nodes_list = ['a19399', 'p556343', 'p758877', 'c2512'] #list(dbis_graph.nodes)[lower_partition_index:upper_partition_index]\n",
    "start_time = time.time()\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    for i, result in enumerate(pool.imap(create_samples_for_node, nodes_list, chunksize=1)):\n",
    "        sim_G_sampling[nodes_list[i]] = result\n",
    "        if (i+1) % 400 == 0:\n",
    "            message = \"{}: Finished {}/{} nodes\".format(experiment_name,i+1,len(nodes_list))\n",
    "            print(message)\n",
    "            try:\n",
    "                if SEND_NOTIFICATIONS:\n",
    "                    bot.send_message(chat_id=chat_id, text=message)\n",
    "            except:\n",
    "                print(\"Failed sending message!\")\n",
    "        \n",
    "end_time = time.time()\n",
    "computation_time = end_time - start_time\n",
    "print(\"Whole sampling process took {} sec.\".format(np.around(computation_time, decimals=2)))\n",
    "try:\n",
    "    if SEND_NOTIFICATIONS:\n",
    "        bot.send_message(chat_id=chat_id, text=\"Finished {}: sampling {} nodes for each of {} nodes\".format(experiment_name,samples_per_node, len(nodes_list)))\n",
    "except:\n",
    "    print(\"Failed sending message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save dict with node-id -> similar-nodes-list as pickle file\n",
    "dbis_sampling_v1_file_path = dataset_path + 'dbis_sampling_v1_partition_{}.p'.format(partition_id)\n",
    "with open(dbis_sampling_v1_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(sim_G_sampling, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dict with node-id -> similar-nodes-list from pickle file\n",
    "sim_G_sampling_reload={}\n",
    "with open(dbis_sampling_v1_file_path, 'rb') as pickle_file:\n",
    "    sim_G_sampling_reload = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build nodes x samples_per_node node index matrix for verse c++-implementation \n",
    "node_samples_arr = []\n",
    "for i in range((len(nodes_list))):\n",
    "    node = id_2_node[i]\n",
    "    sampled_nodes = sim_G_sampling_reload[node]\n",
    "    sampled_node_indices = []\n",
    "    for n in sim_G_sampling_reload[node]:\n",
    "        sampled_node_indices.append(node_2_id[n])\n",
    "    node_samples_arr.extend(sampled_node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write node index sample matrix to file\n",
    "node_index_samples_file_path = dataset_path + 'node_index_samples_dbis_v1_partition_{}.smp'.format(partition_id)\n",
    "with open(node_index_samples_file_path, 'wb') as node_index_samples_file:\n",
    "    node_index_samples_file.write(pack('%di' % len(nodes_list)*samples_per_node, *node_samples_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read conference labels from file\n",
    "conference_labels_file_path = dataset_path + 'googlescholar_conference_labels.txt'\n",
    "conference_labels_df = pd.read_csv(conference_labels_file_path, sep=' ', header=None, dtype={0:str, 1:int})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
