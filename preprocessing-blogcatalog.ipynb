{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import pickle\n",
    "import pprint\n",
    "import chardet\n",
    "from telegram import Bot\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from struct import pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pretty printer\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initilize telegram bot\n",
    "token = \"350553078:AAEu70JDqMFcG_x5eBD3nqccTvc4aFNMKkg\"\n",
    "chat_id = \"126551968\"\n",
    "bot = Bot(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPORT_AS_EDGE_LIST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dataset file paths\n",
    "dataset_path = 'data/BlogCatalog-dataset/data/'\n",
    "friend_edges_csv_path = dataset_path + 'edges.csv'\n",
    "group_edges_csv_path = dataset_path + 'group-edges.csv'\n",
    "groups_csv_path = dataset_path + 'groups.csv'\n",
    "bloggers_csv_path = dataset_path + 'nodes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store cvs contents in dataframe\n",
    "friend_edges_df = pd.read_csv(friend_edges_csv_path, sep=',', header=None, dtype={0: str, 1:str})\n",
    "group_edges_df = pd.read_csv(group_edges_csv_path, sep=',', header=None, dtype={0: str, 1:str})\n",
    "groups_df = pd.read_csv(groups_csv_path, sep=',', header=None, dtype={0: str})\n",
    "bloggers_df = pd.read_csv(bloggers_csv_path, sep=',', header=None, dtype={0: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give bloggers and groups unique node-ids\n",
    "bloggers_df[0] = 'b' + bloggers_df[0]\n",
    "friend_edges_df = 'b' + friend_edges_df\n",
    "groups_df[0] = 'g' + groups_df[0]\n",
    "group_edges_df[0] = 'b' + group_edges_df[0]\n",
    "group_edges_df[1] = 'g' + group_edges_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define networkx graph\n",
    "blog_catalog_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define node and edge label constants\n",
    "IS_MEMBER_OF = 'is_member_of'\n",
    "IS_FRIEND_WITH = 'is_friend_with'\n",
    "BLOGGER = 'blogger'\n",
    "GROUP = 'group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add blogger and group nodes to graph\n",
    "blog_catalog_graph.add_nodes_from(bloggers_df[0].tolist(), label=BLOGGER)\n",
    "print(\"{} nodes in graph\".format(blog_catalog_graph.number_of_nodes()))\n",
    "blog_catalog_graph.add_nodes_from(groups_df[0].tolist(), label=GROUP)\n",
    "print(\"{} nodes in graph\".format(blog_catalog_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create edge tuples from dataframe\n",
    "group_edges = list(zip(group_edges_df[0].tolist(), group_edges_df[1].tolist()))\n",
    "friend_edges = list(zip(friend_edges_df[0].tolist(), friend_edges_df[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add (blogger)-[is_member_of]-(group) edges to graph\n",
    "blog_catalog_graph.add_edges_from(group_edges, label=IS_MEMBER_OF)\n",
    "print(\"{} edges in graph\".format(blog_catalog_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(blog_catalog_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add (blogger)-[is_friend_with]-(blogger) edges to graph\n",
    "blog_catalog_graph.add_edges_from(friend_edges, label=IS_FRIEND_WITH)\n",
    "print(\"{} edges in graph\".format(blog_catalog_graph.number_of_edges()))\n",
    "print(\"{} nodes in graph\".format(blog_catalog_graph.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export graph as edge list to given path\n",
    "if EXPORT_AS_EDGE_LIST:\n",
    "    edge_list_export_path = dataset_path + 'blogcatalog_edgelist.csv'\n",
    "    nx.write_edgelist(blog_catalog_graph, edge_list_export_path, data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average degree of all nodes in graph\n",
    "node_degrees = np.array(list(dict(blog_catalog_graph.degree(list(blog_catalog_graph.nodes))).values()),dtype=np.int64)\n",
    "avg_node_degree = np.mean(node_degrees)\n",
    "print(\"The avg. node degree is {}\".format(np.round(avg_node_degree, decimals=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define random walk parameters\n",
    "sim_G_sampling = {}\n",
    "samples_per_node = 10\n",
    "finished_nodes = 0\n",
    "experiment_name = 'Blog-Catalog Node Sampling V1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define meta-path scoring information\n",
    "meta_path_scheme_A = [GROUP, IS_MEMBER_OF, BLOGGER, IS_MEMBER_OF, GROUP]\n",
    "meta_path_scheme_B = [BLOGGER, IS_MEMBER_OF, GROUP, IS_MEMBER_OF, BLOGGER, IS_MEMBER_OF, GROUP]\n",
    "meta_path_schemes = {BLOGGER: [meta_path_scheme_B], GROUP: [meta_path_scheme_A]}\n",
    "scoring_function = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample a meta-path scheme from all meta-path schemes according to given scoring function\n",
    "def sample_meta_path_scheme(node):\n",
    "    node_label = blog_catalog_graph.nodes[node]['label']\n",
    "    meta_path_scheme_index = np.random.choice(list(range(len(meta_path_schemes[node_label]))))\n",
    "    meta_path_scheme = meta_path_schemes[node_label][meta_path_scheme_index]\n",
    "    \n",
    "    return meta_path_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check, wheter neighbor (candidate) of node i in walk fulfills requirements given through meta-path scheme\n",
    "def candidate_valid(node, candidate, meta_path_scheme,step):\n",
    "    node_label_valid = blog_catalog_graph.nodes[candidate]['label'] == meta_path_scheme[step*2]\n",
    "    edge_label_valid = blog_catalog_graph[node][candidate]['label'] == meta_path_scheme[step*2-1]\n",
    "    \n",
    "    return node_label_valid and edge_label_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute transition probabilities for all neighborhood nodes of node i according to given meta-path\n",
    "def compute_transition_probabilities(meta_path_scheme, step, node):\n",
    "    candidate_set = [n for n in blog_catalog_graph.neighbors(node)]\n",
    "    transition_probabilities = np.ones(len(candidate_set), dtype=np.float64)\n",
    "    \n",
    "    for i, candidate in enumerate(candidate_set):\n",
    "        if not candidate_valid(node, candidate, meta_path_scheme, step):\n",
    "            transition_probabilities[i] = 0\n",
    "            \n",
    "    transition_probabilities = transition_probabilities / np.sum(transition_probabilities)\n",
    "    \n",
    "    return transition_probabilities\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run single random walk with transistion probabilities accoring to scoring function\n",
    "def run_single_random_walk(start_node):\n",
    "    visited_nodes = []\n",
    "    current_node = start_node\n",
    "    meta_path_scheme = sample_meta_path_scheme(start_node)\n",
    "    walk_length = int((len(meta_path_scheme) - 1) / 2)\n",
    "    \n",
    "    for i in range(1,walk_length+1):\n",
    "        visited_nodes.append(current_node)\n",
    "        transition_probabilities = compute_transition_probabilities(meta_path_scheme, i, current_node)\n",
    "        \n",
    "        if np.sum(transition_probabilities) == 0:\n",
    "            return visited_nodes\n",
    "        current_node = np.random.choice([n for n in blog_catalog_graph.neighbors(current_node)], p=transition_probabilities)\n",
    "    \n",
    "    return visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample 10.000 times a similar node given particular node\n",
    "def create_samples_for_node(node):\n",
    "    sampled_nodes = []\n",
    "    \n",
    "    for i in range(samples_per_node):\n",
    "        sampled_nodes.append(run_single_random_walk(node)[-1])\n",
    "    \n",
    "    return sampled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10.000 similar nodes for each node in node_list in parallel\n",
    "nodes_list = list(blog_catalog_graph.nodes)\n",
    "start_time = time.time()\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    for i, result in enumerate(pool.imap(create_samples_for_node, nodes_list, chunksize=1)):\n",
    "        sim_G_sampling[nodes_list[i]] = result\n",
    "        if (i+1) % 400 == 0:\n",
    "            message = \"{}: Finished {}/{} nodes\".format(experiment_name,i+1,len(nodes_list))\n",
    "            print(message)\n",
    "            try:\n",
    "                bot.send_message(chat_id=chat_id, text=message)\n",
    "            except:\n",
    "                print(\"Failed sending message!\")\n",
    "        \n",
    "end_time = time.time()\n",
    "computation_time = end_time - start_time\n",
    "print(\"Whole sampling process took {} sec.\".format(np.around(computation_time, decimals=2)))\n",
    "try:\n",
    "    bot.send_message(chat_id=chat_id, text=\"Finished {}: sampling {} nodes for each of {} nodes\".format(experiment_name,samples_per_node, len(nodes_list)))\n",
    "except:\n",
    "    print(\"Failed sending message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save dict with node-id -> similar-nodes-list as pickle file\n",
    "blogcatalog_sampling_v1_file_path = dataset_path + 'blogcatalog_sampling_v1.p'\n",
    "with open(blogcatalog_sampling_v1_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(sim_G_sampling, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dict with node-id -> similar-nodes-list from pickle file\n",
    "blogcatalog_sampling_v1_file_path = 'results/blogcatalog/blogcatalog_sampling_v1.p'\n",
    "sim_G_sampling_reload={}\n",
    "with open(blogcatalog_sampling_v1_file_path, 'rb') as pickle_file:\n",
    "    sim_G_sampling_reload = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate to how many groups each blogger belongs to\n",
    "groups_count = []\n",
    "for n in list(blog_catalog_graph.nodes):\n",
    "    if 'b' in n:\n",
    "        if int(n.split('b')[-1]) % 1000 == 0:\n",
    "            print(n)\n",
    "        groups_count.append(np.sum(group_edges_df[0] == n))\n",
    "        \n",
    "groups_count = np.array(groups_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate to how many groups a blogger belongs to in average\n",
    "print(\"Avg. groups per blogger: {}\".format(np.mean(groups_count)))\n",
    "\n",
    "# calculate how many bloggers belong to no group\n",
    "print(\"Number of bloggers without group: {}\".format(np.sum(groups_count == 0)))\n",
    "\n",
    "# calculate how many bloggers belong to only one group\n",
    "print(\"Number of bloggers with only one group: {}\".format(np.sum(groups_count == 1)))\n",
    "\n",
    "# calculate how many bloggers belong to two groups\n",
    "print(\"Number of bloggers with two groups: {}\".format(np.sum(groups_count == 2)))\n",
    "\n",
    "# calculate how many bloggers belong to more than two groups\n",
    "print(\"Number of bloggers with more than two groups: {}\".format(np.sum(groups_count > 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load id-to-node mapping of verse embeddings\n",
    "id2node_filepath = 'data/BlogCatalog-dataset/data/blogcatalog_mapping_ids_to_nodes.p'\n",
    "id_2_node = {}\n",
    "with open(id2node_filepath, 'rb') as id_2_node_file:\n",
    "    id_2_node = pickle.load(id_2_node_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load node-to-id mapping of verse embeddings\n",
    "node2id_filepath = 'data/BlogCatalog-dataset/data/blogcatalog_mapping_nodes_to_ids.p'\n",
    "node_2_id = {}\n",
    "with open(node2id_filepath, 'rb') as node_2_id_file:\n",
    "    node_2_id = pickle.load(node_2_id_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build nodes x samples_per_node node index matrix for verse c++-implementation \n",
    "node_samples_arr = []\n",
    "for i in range((len(nodes_list))):\n",
    "    node = id_2_node[i]\n",
    "    sampled_nodes = sim_G_sampling_reload[node]\n",
    "    sampled_node_indices = []\n",
    "    for n in sim_G_sampling_reload[node]:\n",
    "        sampled_node_indices.append(node_2_id[n])\n",
    "    node_samples_arr.extend(sampled_node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write node index sample matrix to file\n",
    "node_index_samples_file_path = dataset_path + 'node_index_samples_blogcatalog_v1.smp'\n",
    "with open(node_index_samples_file_path, 'wb') as node_index_samples_file:\n",
    "    node_index_samples_file.write(pack('%di' % len(nodes_list)*samples_per_node, *node_samples_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
